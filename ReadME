# Efficient Attention with Selection and Merging Mechanism

This repository provides an implementation of an efficient attention mechanism based on selection and merging strategies. Our method achieves competitive performance while significantly reducing memory requirements, enabling models like Llama and Mistral to extend context lengths to nearly 1M tokens. The following sections detail the pre-training, fine-tuning, and inference phases, as well as the controllable fitting of the loss curve.

---
## Pre-training

### Overview
Our attention mechanism incorporates a correlation-aware selection strategy that optimizes token selection during the attention computation. This allows for efficient sparse attention, enhancing performance on long sequences.

- **Positional Encodings**: We design a novel positional encoding scheme that generalizes across unseen positions during pre-training.
- **Token Selection**: During pre-training, similar queries (Q) are grouped and share key-value (KV) pairs for computational efficiency.

### Pre-training Script
To begin pre-training, run the following script:
```bash
python pretrain.py --config config/pretrain.yaml
```

---

## Fine-tuning

### Overview
We leverage the selection and merging mechanism to fine-tune models on sequence lengths of up to 16K tokens. Our Cyclic, Randomly Truncated, and Dynamically Growing NTK Position Encoding (CRD NTK) reduces memory usage while maintaining performance comparable to full attention mechanisms.

- **CRD NTK Positional Encoding**: This technique allows the model to generalize positional information efficiently during fine-tuning.
- **Efficient Fine-tuning**: By selecting a subset of tokens for key-value attention, we reduce memory usage and accelerate the fine-tuning process on large models.

### Fine-tuning Script
To fine-tune the model, use:
```bash
python finetune.py --config config/finetune.yaml
```

---

## Inference

### Overview
Our approach supports scalable inference with long sequences, up to 1M tokens, by leveraging dynamically growing positional encodings and efficient token selection during attention computation.

- **Positional Extrapolation**: We achieve high-ratio positional extrapolation by combining NTK-aware encodings and token selection strategies. The method ensures minimal performance loss over long sequences.
- **Low Memory Usage**: Our attention mechanism requires only a fraction of the memory compared to full attention methods.

### Inference Script
To perform inference on long sequences, run:
```bash
python infer.py --config config/infer.yaml
```

---

## Controllable Loss Curve

### Overview
Our method introduces a degree of controllability over the model's fitting capability by adjusting the number of selected tokens and compression levels. This allows fine-grained control over the trade-off between performance and memory efficiency.

- **Controllable Fitting**: By tuning the selection and merging parameters, the loss curve can be adjusted to fit the model's needs for different tasks.
- **Multi-scale Compression**: Future work will incorporate multi-scale compression for even greater control over model fitting.

### Loss Curve Example
![Loss Curve](path_to_loss_curve_image)

---

## Future Work and Open Source

The complete codebase for pre-training, fine-tuning, and inference will be open-sourced in the upcoming weeks. We are currently refining the implementation and performing additional experiments. Please stay tuned for updates.

For now, feel free to explore the repository, review the pre-training, fine-tuning, and inference scripts, and check out our preliminary results.

---

Stay tuned for more updates and the full release!

---

